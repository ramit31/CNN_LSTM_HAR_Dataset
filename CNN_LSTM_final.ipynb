{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## About the Dataset\n",
    "The dataset set used is called Human Activity Recognition(HAR) in which the experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, they captured *3-axial linear acceleration* and *3-axial angular velocity* at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n",
    "\n",
    "The sensor signals *(accelerometer and gyroscope)* were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used.\n",
    "\n",
    "## Objective\n",
    "To train a model on sensor data from accelerometer and gyroscope having spacial information in 3 axis and temporal information to produce output in the form of one of the 6 classes described in the dataset.\n",
    "\n",
    "## Implementation for cnn-lstm\n",
    "There were three variables which were measured in three axis x,y,z - total acceleration, body acceleration and angular acceleration from gyroscope. These resulted in 9 files with each file having samples having 128 time steps. The data is combined from all the files each variable acting as features and converting the entire dataset into shape *no. of samples X time steps X no. features*. For training data, this shape is 7352 X 128 X 9. For each sample there is an output out of 6 activities as stated above. This ouput data is then converted into one-hot encoded data of an array of size 6.\n",
    "\n",
    "This data is then converted into a 4D matrix for passing into the CNN architecture. **The objective of CNN architecture** is find pattern across the 3 variables and in their 3 axis & also see if there is correlation between nearby data points. To achieve this, the data is converted into the format of *no. of samples X length X width X channels*. For this dataset, the this converts into 7352 X 4 X 32 X 9. This means that for each sample, the 128 points are divided into 32 batch size and stacked one below another. 1D convolutional layers are used in this expermient because the sequence is 1D and 2D convolutional layer will not provide much meaningful information. Padding is not same for the output to avoid creating dummy data points to fill the size. For all purposes of discussion the input size taken will be for one subsequence, i.e., 1 X 1 X 32 X 9. The input is passed through 2 1D convolutional layers with kernel size 4 and no. of filters 64 and 128 respectively which produces ouput data of size 1 X 1 X 29 X 64 & 1 X 1 X 26 X 128 respectively. It is then passed through dropout layer to keep units with having 0.6 or more probability. This layer acts as regularization in the architecture to avoid overfitting. This is passed through max pool layer to reduce the no. of data points and consolidating data with output size 1 X 1 X 13 X 128 and finally through flatten layer to convert the 3 dimensions - length, width and channels into a single matrix of size 1 X 1 X 1664. This is the end of CNN pipeline.\n",
    "\n",
    "This is passed through time distributed layer to make each flattened layer as an individual time step data otherwise the data would have been converted into the form 1 X 1664. After this it is passed through LSTM layer. **LSTM layer's objective** is to get temporal information so the data will be passed in sample each having some time steps having 1664 features. This layer has 100 units with ouput size 1 X 100 where each layer is producing hidden cell information for LSTM. This data is passed through dropout layer with probability 0.6. It is then passed through dense layer having 125 units and finally through output layer having 6 layers to produce ouput for one of the 6 classes.\n",
    "\n",
    "## Model Used\n",
    "The data was tested on two models - one having purely LSTM layer and another having CNN and LSTM layers. Both were trained on same no. of epochs, i.e., 15 and batch size 64. It was found that accuracy for LSTM model was found to be 88.8%(avg) and for CNN-LSTM model it was 90.6%(avg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "78NTS9cmNsKk",
    "outputId": "8d0d745f-cd60-47ab-a5f0-6ae1b8648526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# For running in google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqQY83lfNyI5"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "from zipfile import ZipFile\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZaFsB_u5d1dL",
    "outputId": "88ea9054-af1a-4f7b-92ca-6af730adc692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UCI HAR Dataset: 61.0MB [00:01, 45.1MB/s]                           \n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    check if the data (zip) file is already downloaded\n",
    "    if not, download it from \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\" \n",
    "    and save as UCI HAR Dataset.zip\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "folder_path = 'UCI HAR Dataset'\n",
    "\n",
    "class DownloadProgress(tqdm):\n",
    "\"\"\"\n",
    "    A class used to create tqdm object whichc is used to pass to the urlretrieve method to show download meter\n",
    "    \n",
    "    Atributes:\n",
    "    ----------\n",
    "    last_block: int\n",
    "        no. of blocks transferred last\n",
    "    block_num: int\n",
    "        no. of block transferred\n",
    "    block_size: float\n",
    "        size of bloack in bytes\n",
    "    total_size: float\n",
    "        stores total size of the file\n",
    "\"\"\"\n",
    "    \n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='UCI HAR Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip',\n",
    "            'UCI HAR Dataset.zip',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(folder_path):\n",
    "    with ZipFile('UCI HAR Dataset.zip') as zip:\n",
    "        zip.extractall()\n",
    "        zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwJuscjbP7xA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Loads the data from the filepath as dataframe and returns an array of values\n",
    "\"\"\"\n",
    "def load_file(filepath):\n",
    "    # load a single file as a numpy array\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rf_gSDSfKzO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Loads groups of data from the filepath into an array and returns reshaped version of array\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filenames: list\n",
    "        list of tring containing the file names\n",
    "    prefix: str\n",
    "        stores the path upto the file name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loaded: list\n",
    "        Array of transformed data\n",
    "\"\"\"\n",
    "def load_group(filenames, prefix=''):\n",
    "    # load a list of files and return as a 3d numpy array\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHlcYArMfVhG"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Loads the train and test dataset\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    group: str\n",
    "        string containing 'train' or 'test'\n",
    "    prefix: str\n",
    "        stores the path upto the file name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X,y: list\n",
    "        Array of input and output data\n",
    "\"\"\"\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    # load a dataset group, such as train or test\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntONiTLMfVkl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Loads the train and test input and ouput data and convert the ouput into one-hot encoded form\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    prefix: str\n",
    "        stores the path upto the file name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    trainX: list\n",
    "        Array of training input\n",
    "    trainy: list\n",
    "        Array of training output\n",
    "    testX: list\n",
    "        Array of testing input\n",
    "    testy: list\n",
    "        Array of testing output\n",
    "\"\"\"\n",
    "def load_dataset(prefix=''):\n",
    "    # load the dataset, returns train and test X and y elements\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'UCI HAR Dataset/')\n",
    "\tprint(trainX.shape, trainy.shape)\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'UCI HAR Dataset/')\n",
    "\tprint(testX.shape, testy.shape)\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\treturn trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qzKZx20fVoN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Build the LSTM neural network, fit on training data and test the accuracy of model.\n",
    "    The neural network architecture has four layers: LSTM layer, Dropout Layer, Dense layer, Ouput Layer\n",
    "    For evaluation -\n",
    "    No of epochs: 15\n",
    "    Batch size: 64\n",
    "    Loss function: categorical cross entropy(i.e. combination of softmax and cross entropy function)\n",
    "    Optimizer used: Adam\n",
    "    Metric for accuracy: accuracy against truth value\n",
    "    Architecture -\n",
    "    LSTM layer:\n",
    "        no. of units: 100\n",
    "        Input shape: no. of samples in each sliding frame X no. of features\n",
    "    Dropout layer:\n",
    "        probability for keeping the unit: 0.5\n",
    "    Dense layer:\n",
    "        no. of units: 100\n",
    "        activation function: linear rectifier\n",
    "    Output layer:\n",
    "        no. of units: 6\n",
    "        activation function: softmax\n",
    "\"\"\"\n",
    "def evaluate_model_lstm(trainX, trainy, testX, testy):\n",
    "    # fit and evaluate a model\n",
    "    #lstm model\n",
    "\tverbose, epochs, batch_size = 0, 15, 64\n",
    "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yj1bHt1wN2A8"
   },
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "#cnn lstm model\n",
    "\"\"\"\n",
    "    Build the CNN LSTM neural network, fit on training data and test the accuracy of model.\n",
    "    The neural network architecture has 10 layers: 1D Convolution layer, 1D Convolution layer,\n",
    "    Dropout layer, 1D Maxpool layer, Time Distributed Layer, Flatten layer, LSTM layer,\n",
    "    Dropout Layer, Dense layer, Ouput Layer\n",
    "    For evaluation -\n",
    "    No of epochs: 25\n",
    "    Batch size: 64\n",
    "    Loss function: categorical cross entropy(i.e. combination of softmaxx and cross entropy function)\n",
    "    Optimizer used: Adam\n",
    "    Metric for accuracy: accuracy against truth value\n",
    "    Architecture -\n",
    "    1D Convolution layer:\n",
    "        converted sliding frames X sample X features into no. of inputs X length X width X features\n",
    "        no. of filters: 64\n",
    "        kernel size: 4\n",
    "        stride: 1\n",
    "        activation function: linear rectifier\n",
    "        input shape: length X width X no. of features\n",
    "    1D Convolution layer:\n",
    "        no. of filters: 128\n",
    "        kernel size: 4\n",
    "        stride: 1\n",
    "        activation function: linear rectifier\n",
    "    Dropout layer:\n",
    "        probability for keeping the unit: 0.6\n",
    "    1D Maxpool layer:\n",
    "        kernel size: 2\n",
    "    Flatten layer:\n",
    "        Flatten the 4D ouput from convolutional layers\n",
    "    Time Distributed Layer:\n",
    "        a wrapper around the CNN layers to make a temporal slice of the outputs from CNN and not convert them into a\n",
    "        single input data\n",
    "    LSTM layer:\n",
    "        no. of units: 100\n",
    "        activation function: tanh\n",
    "    Dropout layer:\n",
    "        probability for keeping the unit: 0.6\n",
    "    Dense layer:\n",
    "        no. of units: 125\n",
    "        activation function: linear rectifier\n",
    "    Output layer:\n",
    "        no. of units: 6\n",
    "        activation function: softmax\n",
    "\"\"\"\n",
    "def evaluate_model_cnn_lstm(trainX, trainy, testX, testy):\n",
    "\t# define model\n",
    "\tverbose, epochs, batch_size = 0, 25, 64\n",
    "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\t# reshape data into time steps of sub-sequences\n",
    "\tn_steps, n_length = 4, 32\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "\ttestX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=4, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "\tmodel.add(TimeDistributed(Conv1D(filters=128, kernel_size=4, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dropout(0.6)))\n",
    "\tmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "\tmodel.add(TimeDistributed(Flatten()))\n",
    "\tmodel.add(LSTM(100))\n",
    "\tmodel.add(Dropout(0.6))\n",
    "\tmodel.add(Dense(125, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtCds1FwfVrB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prints the mean and standard deviation of the scores calculated\n",
    "    \n",
    "    Parameters:\n",
    "    scores: list\n",
    "        array of scores\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    None\n",
    "\"\"\"\n",
    "def summarize_results(scores):\n",
    "    # summarize scores\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZ4zxHNqh7v3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs the LSTM experiment with compiling, evaluating and displaying the results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    repeats: int\n",
    "        no. of time the model needs to be evaluated\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    None\n",
    "\"\"\"\n",
    "def run_experiment_lstm(repeats=5):\n",
    "    # run a lstm experiment\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model_lstm(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\t# summarize results\n",
    "\tsummarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqG9Gk0yfVts"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs the CNN-LSTM experiment with compiling, evaluating and displaying the results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    repeats: int\n",
    "        no. of time the model needs to be evaluated\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    None\n",
    "\"\"\"\n",
    "def run_experiment_cnn_lstm(repeats=5):\n",
    "    # run a cnn-lstm experiment\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model_cnn_lstm(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\t# summarize results\n",
    "\tsummarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "DDWSL_0lfVxG",
    "outputId": "307b835b-4d72-4cc3-b70a-c009fff8b1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      ">#1: 89.379\n",
      ">#2: 89.888\n",
      ">#3: 88.768\n",
      ">#4: 88.904\n",
      ">#5: 91.381\n",
      "[89.37902952154734, 89.88802171700034, 88.76823888700373, 88.90397013912454, 91.38106549032915]\n",
      "Accuracy: 89.664% (+/-0.944)\n"
     ]
    }
   ],
   "source": [
    "# run the lstm experiment\n",
    "run_experiment_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "PyxuvonclvvM",
    "outputId": "52c32607-fb1b-426d-b158-ae7b7e2c7346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      ">#1: 91.754\n",
      ">#2: 90.668\n",
      ">#3: 88.191\n",
      ">#4: 91.144\n",
      ">#5: 90.363\n",
      "[91.75432643366135, 90.66847641669494, 88.19138106549033, 91.14353579911774, 90.36308109942314]\n",
      "Accuracy: 90.424% (+/-1.211)\n"
     ]
    }
   ],
   "source": [
    "#run the cnn lstm experiment\n",
    "run_experiment_cnn_lstm()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
